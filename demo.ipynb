{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d223b370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m[WARN ] Dependency `gsplat` is required for rendering 3DGS. Install via: pip install git+https://github.com/nerfstudio-project/gsplat.git@0b4dddf04cb687367602c01196913cde6a743d70\u001b[0m\n",
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pose results keys : dict_keys(['keypoints', 'scores', 'labels', 'bbox']) \n",
      " labels = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from main_pipeline import BodyReconstructionPipeline\n",
    "import config\n",
    "\n",
    "\n",
    "\n",
    "if config.FORCE_CPU:\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = BodyReconstructionPipeline(\n",
    "    device=device,\n",
    "    output_dir=config.OUTPUT_DIR\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e08650f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/5] Estimating depth and camera FOV...\n",
      "\u001b[97m[INFO ] using MLP layer as FFN\u001b[0m\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.0370023250579834 seconds. Shape:  torch.Size([1, 3, 504, 378])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.17250394821166992 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0006089210510253906 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Export Results Done. Time: 0.0003056526184082031 seconds\u001b[0m\n",
      "########### Using fov estimator: MoGe2...\n",
      "✓ Depth map shape: (504, 378)\n",
      "✓ Camera intrinsics estimated\n",
      "\n",
      "[3/5] Generating segmentation masks...\n",
      "✓ Generated 0 feet mask(s)\n",
      "✓ Generated 1 mask(s)\n",
      "\n",
      "[4/5] Creating 3D point cloud...\n",
      "  Applying DBSCAN clustering to clean point cloud...\n",
      "  ✓ Removed 20 outlier points, kept 65916 points\n",
      "✓ Point cloud created: 65916 points\n",
      "\n",
      "Cleaning up GPU memory...\n",
      "✓ GPU memory freed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Depth Estimation\n",
    "depth_map, processed_image, cam_intrinsics = pipeline._estimate_depth_and_fov(\"mohamed.jpg\")\n",
    "pipeline.cam_intrinsics = cam_intrinsics\n",
    "\n",
    "# Step 2: 2D Pose Detection\n",
    "# keypoints_2d = self._detect_pose_2d(image_path, processed_image)\n",
    "# keypoints_2d = pipeline.infer_pose(processed_image)\n",
    "\n",
    "# Step 3: Segmentation (SAM3)\n",
    "masks, boxes, scores = pipeline._generate_segmentation_masks(processed_image)\n",
    "\n",
    "# Step 4: Create 3D Point Cloud\n",
    "point_cloud_array, pcd = pipeline._create_point_cloud(depth_map, processed_image, sam_mask=masks[0])\n",
    "\n",
    "# Clean up GPU memory before fitting\n",
    "pipeline._cleanup_gpu()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae374687",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "NLF-based SMPL-X fitter\n",
    "=======================\n",
    "Wraps the TorchScript NLF model so it can be used with the same interface as\n",
    "`MetricSMPLFitter` inside `main_pipeline`. The NLF model already predicts\n",
    "SMPL-X vertices, so this class focuses on loading the SMPL-X topology for\n",
    "visualization, projecting to images, and providing the z-buffer / chamfer\n",
    "loss against an observed point cloud.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torchvision  # Required for TorchScript model deps\n",
    "import numpy as np\n",
    "import cv2\n",
    "import smplx\n",
    "from typing import Dict, Optional, Tuple\n",
    "from pytorch3d.structures import Pointclouds\n",
    "from pytorch3d.loss import chamfer_distance\n",
    "from smplfitter.pt import BodyModel, BodyFitter\n",
    "from typing import List, Tuple\n",
    "import random\n",
    "\n",
    "\n",
    "class NLFSMPLFitter:\n",
    "    \"\"\"Drop-in replacement for `MetricSMPLFitter` using the NLF TorchScript model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str = \"checkpoints/nlf_l_multi.torchscript\",\n",
    "        smplx_model_path: str = \"./data/smplx\",\n",
    "        gender: str = \"neutral\",\n",
    "        device: str = \"cuda\",\n",
    "        image: Optional[np.ndarray] = None,\n",
    "    ) -> None:\n",
    "        self.device = torch.device(device)\n",
    "        self.model = torch.jit.load(model_path).to(self.device).eval()\n",
    "        self.image = image\n",
    "\n",
    "        # Load SMPL-X just to obtain faces/topology and enable exports/visuals.\n",
    "        self.smplx_model = smplx.create(\n",
    "            model_path=smplx_model_path,\n",
    "            model_type=\"smplx\",\n",
    "            gender=gender,\n",
    "            use_face_contour=False,\n",
    "            num_betas=10,\n",
    "            num_expression_coeffs=10,\n",
    "            ext=\"npz\",\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Optional refinement fitter if we want to re-fit parametric pose/shape.\n",
    "        self.body_model = BodyModel(\n",
    "            \"smplx\",\n",
    "            gender,\n",
    "            num_betas=10,\n",
    "            model_root=f\"{smplx_model_path}/smplx\",\n",
    "        ).to(self.device)\n",
    "        self.body_fitter = BodyFitter(self.body_model).to(self.device)\n",
    "        self.body_fitter = torch.jit.script(self.body_fitter)\n",
    "\n",
    "        self.prediction = None\n",
    "        self.fitted_params: Optional[Dict[str, torch.Tensor]] = None\n",
    "\n",
    "    def _prepare_image_tensor(self, image: np.ndarray) -> torch.Tensor:\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            return image.to(self.device)\n",
    "        tensor = torch.from_numpy(image).permute(2, 0, 1).contiguous().float()\n",
    "        return tensor.to(self.device)\n",
    "\n",
    "    def _infer_nlf(self, image: np.ndarray) -> Dict[str, torch.Tensor]:\n",
    "        image_tensor = self._prepare_image_tensor(image)\n",
    "        frame_batch = image_tensor.unsqueeze(0)\n",
    "        with torch.inference_mode():\n",
    "            pred = self.model.detect_smpl_batched(frame_batch, model_name=\"smplx\")\n",
    "        return pred\n",
    "\n",
    "    def _project_points(self, points_3d: torch.Tensor, cam_intrinsics: torch.Tensor) -> torch.Tensor:\n",
    "        fx = cam_intrinsics[0, 0]\n",
    "        fy = cam_intrinsics[1, 1]\n",
    "        cx = cam_intrinsics[0, 2]\n",
    "        cy = cam_intrinsics[1, 2]\n",
    "        x_2d = fx * points_3d[:, 0] / (points_3d[:, 2] + 1e-6) + cx\n",
    "        y_2d = fy * points_3d[:, 1] / (points_3d[:, 2] + 1e-6) + cy\n",
    "        return torch.stack([x_2d, y_2d], dim=1)\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        keypoints_2d: Optional[np.ndarray] = None,\n",
    "        cam_intrinsics: Optional[np.ndarray] = None,\n",
    "        depth_map: Optional[np.ndarray] = None,\n",
    "        mask: Optional[np.ndarray] = None,\n",
    "        feet_mask: Optional[np.ndarray] = None,\n",
    "        point_cloud: Optional[np.ndarray] = None,\n",
    "        **_: Dict,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Run the NLF model and package outputs to mimic MetricSMPLFitter.\n",
    "        Keypoint arguments are accepted for API compatibility but unused.\n",
    "        \"\"\"\n",
    "        if self.image is None:\n",
    "            raise ValueError(\"Image not provided to NLFSMPLFitter.\")\n",
    "\n",
    "        pred = self._infer_nlf(self.image)\n",
    "        self.prediction = pred\n",
    "\n",
    "        # Extract primary outputs\n",
    "        pose = pred[\"pose\"][0].to(self.device)\n",
    "        betas = pred[\"betas\"][0].to(self.device)\n",
    "        transl = pred[\"trans\"][0].to(self.device)\n",
    "        vertices = pred[\"vertices3d\"][0].to(self.device)\n",
    "        joints = pred[\"joints3d\"][0].to(self.device)\n",
    "\n",
    "        # Optional parametric refinement to align with SMPL-X topology\n",
    "        try:\n",
    "            with torch.inference_mode():\n",
    "                print(f\" shapes of vertices: {vertices.shape}, joints: {joints.shape} \")\n",
    "                fit_res = self.body_fitter.fit(vertices, joints, num_iter=3, beta_regularizer=1)\n",
    "            pose_rotvecs = fit_res.get(\"pose_rotvecs\", pose)\n",
    "            shape_betas = fit_res.get(\"shape_betas\", betas)\n",
    "            trans_fitted = fit_res.get(\"trans\", transl)\n",
    "            print(\"NLF BodyFitter refinement succeeded.\")\n",
    "        except Exception as exc:  # pragma: no cover - defensive fallback\n",
    "            print(f\"NLF BodyFitter refinement failed, using raw predictions: {exc}\")\n",
    "            pose_rotvecs, shape_betas, trans_fitted = pose.unsqueeze(0), betas.unsqueeze(0), transl.unsqueeze(0)\n",
    "\n",
    "        # Store params compatible with smplx forward\n",
    "        self.fitted_params = {\n",
    "            \"betas\": shape_betas,\n",
    "            \"global_orient\": pose_rotvecs[:, :3],\n",
    "            \"body_pose\": pose_rotvecs[:, 3:],\n",
    "            \"transl\": trans_fitted,\n",
    "            \"left_hand_pose\": torch.zeros((1, 6), device=self.device),\n",
    "            \"right_hand_pose\": torch.zeros((1, 6), device=self.device),\n",
    "            \"expression\": torch.zeros((1, 10), device=self.device),\n",
    "            \"vertices\": vertices,\n",
    "            \"joints\": joints,\n",
    "        }\n",
    "\n",
    "        # Compute optional z-buffer chamfer loss for logging\n",
    "        if cam_intrinsics is not None and point_cloud is not None:\n",
    "            cam_intrinsics_torch = torch.from_numpy(cam_intrinsics[0] if cam_intrinsics.ndim == 3 else cam_intrinsics).float().to(self.device)\n",
    "            # _ = self.compute_depth_loss_with_point_cloud(vertices, cam_intrinsics_torch, point_cloud)\n",
    "            # get zbuffer points\n",
    "            zbuffer_points = self.get_zbuffer(vertices, cam_intrinsics_torch)\n",
    "            print(f\" shapes of zbuffer points: {zbuffer_points.shape}, point cloud: {point_cloud.shape} \")\n",
    "\n",
    "        return self.fitted_params\n",
    "\n",
    "    def get_zbuffer(\n",
    "        self,\n",
    "        vertices: torch.Tensor,\n",
    "        cam_intrinsics: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        device = vertices.device\n",
    "        # points_2d = self._project_points(vertices, cam_intrinsics)\n",
    "        print(f\" dimensions of cam_intrinsics: {cam_intrinsics.shape}, vertices: {vertices.shape} \")\n",
    "        print(f\" devices of cam_intrinsics: {cam_intrinsics.device}, vertices: {vertices.device} \")\n",
    "        points_2d = self.project_vertices(vertices, cam_intrinsics.unsqueeze(0)).squeeze(0)\n",
    "        print(f\" dimensions of projected points_2d: {points_2d.shape} \")\n",
    "        vertices = vertices.squeeze(0)\n",
    "        \n",
    "        u = points_2d[:, 0].long()\n",
    "        v = points_2d[:, 1].long()\n",
    "        z = vertices[:, 2]\n",
    "        print(f\" dimensions of u: {u.shape}, v: {v.shape}, z: {z.shape} \")\n",
    "        \n",
    "        H = int(cam_intrinsics[1, 2].item() * 2)\n",
    "        W = int(cam_intrinsics[0, 2].item() * 2)\n",
    "\n",
    "        inside = (u >= 0) & (u < W) & (v >= 0) & (v < H) & (z > 0)\n",
    "        u, v, z = u[inside], v[inside], z[inside]\n",
    "        if z.numel() == 0:\n",
    "            return torch.zeros((), device=device)\n",
    "\n",
    "        pixel_ids = v * W + u\n",
    "        unique_pixels, inverse_indices = torch.unique(pixel_ids, return_inverse=True)\n",
    "        z_min = torch.full((unique_pixels.shape[0],), float(\"inf\"), device=device)\n",
    "        z_min.scatter_reduce_(0, inverse_indices, z, reduce=\"amin\")\n",
    "\n",
    "        v_zbuf = unique_pixels // W\n",
    "        u_zbuf = unique_pixels % W\n",
    "\n",
    "        fx = cam_intrinsics[0, 0]\n",
    "        fy = cam_intrinsics[1, 1]\n",
    "        cx = cam_intrinsics[0, 2]\n",
    "        cy = cam_intrinsics[1, 2]\n",
    "\n",
    "        x_3d = (u_zbuf.float() - cx) * z_min / fx\n",
    "        y_3d = (v_zbuf.float() - cy) * z_min / fy\n",
    "        z_3d = z_min\n",
    "        zbuffer_points = torch.stack([x_3d, y_3d, z_3d], dim=1)\n",
    "        return zbuffer_points\n",
    "    \n",
    "    def project_vertices(self,coords3d, intrinsic_matrix):\n",
    "        # ensure coords3d is (B, N, 3) and intrinsic_matrix is (B, 3, 3) and are on the same device and torch\n",
    "\n",
    "        if not isinstance(coords3d, torch.Tensor) :\n",
    "            coords3d = torch.from_numpy(coords3d).float().to('cuda')\n",
    "        if not isinstance(intrinsic_matrix, torch.Tensor):\n",
    "            intrinsic_matrix = torch.from_numpy(intrinsic_matrix).float().to('cuda')\n",
    "        \n",
    "        coords3d = coords3d.float().to('cuda')\n",
    "        intrinsic_matrix = intrinsic_matrix.float().to('cuda')\n",
    "\n",
    "        projected = coords3d / torch.maximum(\n",
    "            torch.tensor(0.001), torch.tensor(coords3d[..., 2:]))\n",
    "        \n",
    "        return torch.einsum('bnk,bjk->bnj', projected, intrinsic_matrix[..., :2, :])\n",
    "    \n",
    "    def compute_depth_loss_with_point_cloud(\n",
    "        self,\n",
    "        vertices: torch.Tensor,\n",
    "        cam_intrinsics: torch.Tensor,\n",
    "        point_cloud: torch.Tensor,\n",
    "        it: int = 0,\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        zbuffer_points = self.get_zbuffer(vertices, cam_intrinsics)\n",
    "        device = zbuffer_points.device\n",
    "\n",
    "        if not isinstance(point_cloud, torch.Tensor):\n",
    "            point_cloud = torch.from_numpy(point_cloud).float().to(device)\n",
    "        else:\n",
    "            point_cloud = point_cloud.to(device)\n",
    "\n",
    "        pclouds_zbuf = Pointclouds([zbuffer_points])\n",
    "        pclouds_gt = Pointclouds([point_cloud])\n",
    "        loss, _ = chamfer_distance(pclouds_zbuf, pclouds_gt)\n",
    "\n",
    "        if it % 50 == 0:\n",
    "            print(\n",
    "                f\"Point cloud chamfer loss: {loss.item():.4f} \"\n",
    "                f\"(zbuf points: {zbuffer_points.shape[0]}, gt points: {point_cloud.shape[0]})\"\n",
    "            )\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def center_pcd(self, pcd: np.ndarray) -> np.ndarray:\n",
    "        # this function centers the point cloud around the origin\n",
    "        centroid = np.mean(pcd, axis=0)\n",
    "        pcd -= centroid\n",
    "        return pcd\n",
    "    \n",
    "    def ransac_scale(\n",
    "        self,\n",
    "        pairs: List[Tuple[float, float]], iters: int = 10000, tol: float = 0.05\n",
    "        ) -> Tuple[float, int]:\n",
    "        \"\"\"RANSAC for scale s in s * d_sfm = d_metric.\n",
    "\n",
    "        tol: relative tolerance |s*d_sfm - d_metric| <= tol * d_metric\n",
    "        Returns best_scale, inlier_count.\n",
    "        \"\"\"\n",
    "        if not pairs:\n",
    "            raise ValueError(\"No depth pairs provided.\")\n",
    "        pairs_arr = np.array(pairs, dtype=float)  # (N,2)\n",
    "        d_sfm = pairs_arr[:, 0]\n",
    "        d_met = pairs_arr[:, 1]\n",
    "        ratios = d_met / np.clip(d_sfm, 1e-9, None)\n",
    "        best_s = np.median(ratios)\n",
    "        best_inliers = 0\n",
    "        n = len(pairs)\n",
    "        for _ in range(iters):\n",
    "            i = random.randint(0, n - 1)\n",
    "            s_candidate = d_met[i] / max(d_sfm[i], 1e-9)\n",
    "            pred = s_candidate * d_sfm\n",
    "            err = np.abs(pred - d_met)\n",
    "            inliers = np.sum(err <= tol * np.maximum(d_met, 1e-6))\n",
    "            if inliers > best_inliers:\n",
    "                best_inliers = inliers\n",
    "                best_s = s_candidate\n",
    "\n",
    "        return best_s, best_inliers\n",
    "\n",
    "\n",
    "    def run_metric_optimization(\n",
    "        self,\n",
    "        cam_intrinsics: torch.Tensor,\n",
    "        point_cloud: torch.Tensor,\n",
    "        num_iters: int = 100,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # Placeholder for potential metric optimization steps\n",
    "        # Currently returns the original vertices and joints without modification\n",
    "\n",
    "        # get zbuffer points from vertices\n",
    "        # device = vertices.device\n",
    "        try:\n",
    "            vertices = self.fitted_params[\"vertices\"]\n",
    "            joints = self.fitted_params[\"joints\"]\n",
    "        except Exception as exc:\n",
    "            raise ValueError(\"Fitted parameters not available for metric optimization.\") from exc\n",
    "\n",
    "        zbuffer_points = self.get_zbuffer(vertices, cam_intrinsics)\n",
    "        ## ensure zbuffer are numpy arrays\n",
    "        if not isinstance(zbuffer_points, np.ndarray):\n",
    "            zbuffer_points_np = zbuffer_points.detach().cpu().numpy()\n",
    "        if not isinstance(point_cloud, np.ndarray):\n",
    "            point_cloud = point_cloud.detach().cpu().numpy()\n",
    "            \n",
    "        # point_cloud_np = point_cloud.detach().cpu().numpy()\n",
    "        # center both point clouds\n",
    "        zbuffer_points_np = self.center_pcd(zbuffer_points_np)\n",
    "        point_cloud = self.center_pcd(point_cloud)\n",
    "\n",
    "        # pass only the z values of both point clouds\n",
    "        z_depth = zbuffer_points_np[:, 2]\n",
    "        pc_depth = point_cloud[:, 2]\n",
    "        ## pass both point clouds to ransac scale\n",
    "        scale, inliers = self.ransac_scale(\n",
    "            list(zip(z_depth, pc_depth)), iters=1000, tol=0.1\n",
    "        )\n",
    "        print(f\" RANSAC scale: {scale:.4f} with {inliers} inliers \")\n",
    "        # scale the vertices and joints\n",
    "        # vertices = vertices * scale\n",
    "        # joints = joints * scale\n",
    "\n",
    "        # ## fitted params update\n",
    "        # self.fitted_params[\"vertices\"] = vertices * scale\n",
    "        # self.fitted_params[\"joints\"] = joints * scale\n",
    "\n",
    "        # save the point clouds for visualization\n",
    "        # np.save(\"zbuffer_points.npy\", zbuffer_points_np * scale)\n",
    "        # np.save(\"point_cloud.npy\", point_cloud)\n",
    "        # pcd = np.concatenate([zbuffer_points_np * scale, point_cloud], axis=0)\n",
    "        # np.save(\"combined_pcd.npy\", pcd)\n",
    "\n",
    "        # save it using open3d \n",
    "        import open3d as o3d\n",
    "        zbuf_pcd_o3d = o3d.geometry.PointCloud()\n",
    "        zbuf_pcd_o3d.points = o3d.utility.Vector3dVector(zbuffer_points_np * scale)\n",
    "        o3d.io.write_point_cloud(\"zbuffer_points.ply\", zbuf_pcd_o3d)\n",
    "        pc_pcd_o3d = o3d.geometry.PointCloud()\n",
    "        pc_pcd_o3d.points = o3d.utility.Vector3dVector(point_cloud)\n",
    "        o3d.io.write_point_cloud(\"point_cloud.ply\", pc_pcd_o3d)\n",
    "\n",
    "        combined_pcd_o3d = o3d.geometry.PointCloud()\n",
    "        combined_pcd_o3d.points = o3d.utility.Vector3dVector(\n",
    "            np.concatenate([zbuffer_points_np * scale, point_cloud], axis=0)\n",
    "        )\n",
    "        o3d.io.write_point_cloud(\"combined_pcd.ply\", combined_pcd_o3d)\n",
    "\n",
    "        return {\n",
    "            \"vertices\": vertices,\n",
    "            \"joints\": joints,\n",
    "        }\n",
    "\n",
    "    def project_mesh_on_image(\n",
    "        self,\n",
    "        params: Optional[Dict[str, torch.Tensor]],\n",
    "        image: np.ndarray,\n",
    "        cam_intrinsics: np.ndarray,\n",
    "    ) -> np.ndarray:\n",
    "        if params is None:\n",
    "            params = self.fitted_params\n",
    "        if params is None:\n",
    "            raise ValueError(\"No fitted parameters available for projection.\")\n",
    "\n",
    "        if \"vertices\" in params:\n",
    "            vertices = params[\"vertices\"].detach().cpu().numpy()\n",
    "            vertices = vertices.squeeze(0)\n",
    "            print(f\" using vertices from params with shape: {vertices.shape} \")\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                output = self.smplx_model(**params, return_verts=True)\n",
    "            vertices = output.vertices[0].cpu().numpy()\n",
    "\n",
    "        faces = self.smplx_model.faces\n",
    "\n",
    "        cam = cam_intrinsics[0] if cam_intrinsics.ndim == 3 else cam_intrinsics\n",
    "        fx, fy, cx, cy = cam[0, 0], cam[1, 1], cam[0, 2], cam[1, 2]\n",
    "\n",
    "        vertices_2d = np.zeros((len(vertices), 2))\n",
    "        print(f\" length of vertices: {len(vertices)} \")\n",
    "        for i, v in enumerate(vertices):\n",
    "            if v[2] > 0:\n",
    "                vertices_2d[i, 0] = fx * v[0] / v[2] + cx\n",
    "                vertices_2d[i, 1] = fy * v[1] / v[2] + cy\n",
    "\n",
    "        overlay = image.copy()\n",
    "        for face in faces[::10]:\n",
    "            pts = vertices_2d[face].astype(np.int32)\n",
    "            if np.all((pts[:, 0] >= 0) & (pts[:, 0] < image.shape[1]) & (pts[:, 1] >= 0) & (pts[:, 1] < image.shape[0])):\n",
    "                cv2.polylines(overlay, [pts], True, (0, 255, 0), 1)\n",
    "        return overlay\n",
    "\n",
    "    def get_mesh(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        if self.fitted_params is None:\n",
    "            raise ValueError(\"No fitted parameters. Run fit() first.\")\n",
    "        if \"vertices\" in self.fitted_params:\n",
    "            vertices = self.fitted_params[\"vertices\"].detach().cpu().numpy()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                output = self.smplx_model(**self.fitted_params, return_verts=True)\n",
    "            vertices = output.vertices[0].cpu().numpy()\n",
    "        faces = self.smplx_model.faces\n",
    "        return vertices, faces\n",
    "\n",
    "    def export_mesh(self, output_path: str, params: Optional[Dict[str, torch.Tensor]] = None) -> None:\n",
    "        import trimesh\n",
    "        if params is None:\n",
    "            params = self.fitted_params\n",
    "        if params is None:\n",
    "            raise ValueError(\"No parameters provided for export.\")\n",
    "\n",
    "        # Temporarily swap fitted params if caller supplied a different one\n",
    "        original = self.fitted_params\n",
    "        self.fitted_params = params\n",
    "        verts, faces = self.get_mesh()\n",
    "        self.fitted_params = original\n",
    "        trimesh.Trimesh(vertices=verts, faces=faces).export(output_path)\n",
    "        print(f\"Mesh exported to {output_path}\")\n",
    "\n",
    "\n",
    "__all__ = [\"NLFSMPLFitter\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b35365f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded SMPLX model from ./data/smplx/smplx/SMPLX_NEUTRAL.npz \n",
      " smpl data keys: ['hands_meanr', 'hands_meanl', 'lmk_bary_coords', 'vt', 'posedirs', 'part2num', 'hands_coeffsr', 'lmk_faces_idx', 'J_regressor', 'dynamic_lmk_faces_idx', 'hands_componentsr', 'shapedirs', 'dynamic_lmk_bary_coords', 'ft', 'hands_componentsl', 'joint2num', 'v_template', 'allow_pickle', 'f', 'hands_coeffsl', 'kintree_table', 'weights'] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(504, 378, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitter = NLFSMPLFitter(image=processed_image)\n",
    "processed_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bb32b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " shapes of vertices: torch.Size([1, 10475, 3]), joints: torch.Size([1, 55, 3]) \n",
      " selector for part 0: 434 vertices\n",
      " selector for part 1: 245 vertices\n",
      " selector for part 2: 226 vertices\n",
      " selector for part 3: 153 vertices\n",
      " selector for part 4: 195 vertices\n",
      " selector for part 5: 204 vertices\n",
      " selector for part 6: 186 vertices\n",
      " selector for part 7: 254 vertices\n",
      " selector for part 8: 254 vertices\n",
      " selector for part 9: 614 vertices\n",
      " selector for part 12: 149 vertices\n",
      " selector for part 13: 111 vertices\n",
      " selector for part 14: 123 vertices\n",
      " selector for part 15: 3345 vertices\n",
      " selector for part 16: 276 vertices\n",
      " selector for part 17: 278 vertices\n",
      " selector for part 18: 228 vertices\n",
      " selector for part 19: 230 vertices\n",
      " selector for part 20: 200 vertices\n",
      " selector for part 21: 198 vertices\n",
      " selector for part 22: 320 vertices\n",
      " selector for part 23: 546 vertices\n",
      " selector for part 24: 546 vertices\n",
      " selector for part 25: 33 vertices\n",
      " selector for part 26: 34 vertices\n",
      " selector for part 27: 54 vertices\n",
      " selector for part 28: 35 vertices\n",
      " selector for part 29: 30 vertices\n",
      " selector for part 30: 52 vertices\n",
      " selector for part 31: 23 vertices\n",
      " selector for part 32: 39 vertices\n",
      " selector for part 33: 52 vertices\n",
      " selector for part 34: 30 vertices\n",
      " selector for part 35: 38 vertices\n",
      " selector for part 36: 54 vertices\n",
      " selector for part 37: 21 vertices\n",
      " selector for part 38: 31 vertices\n",
      " selector for part 39: 54 vertices\n",
      " selector for part 40: 33 vertices\n",
      " selector for part 41: 34 vertices\n",
      " selector for part 42: 54 vertices\n",
      " selector for part 43: 35 vertices\n",
      " selector for part 44: 30 vertices\n",
      " selector for part 45: 52 vertices\n",
      " selector for part 46: 23 vertices\n",
      " selector for part 47: 39 vertices\n",
      " selector for part 48: 52 vertices\n",
      " selector for part 49: 30 vertices\n",
      " selector for part 50: 38 vertices\n",
      " selector for part 51: 54 vertices\n",
      " selector for part 52: 21 vertices\n",
      " selector for part 53: 31 vertices\n",
      " selector for part 54: 54 vertices\n",
      " selector for part 0: 434 vertices\n",
      " selector for part 1: 245 vertices\n",
      " selector for part 2: 226 vertices\n",
      " selector for part 3: 153 vertices\n",
      " selector for part 4: 195 vertices\n",
      " selector for part 5: 204 vertices\n",
      " selector for part 6: 186 vertices\n",
      " selector for part 7: 254 vertices\n",
      " selector for part 8: 254 vertices\n",
      " selector for part 9: 614 vertices\n",
      " selector for part 12: 149 vertices\n",
      " selector for part 13: 111 vertices\n",
      " selector for part 14: 123 vertices\n",
      " selector for part 15: 3345 vertices\n",
      " selector for part 16: 276 vertices\n",
      " selector for part 17: 278 vertices\n",
      " selector for part 18: 228 vertices\n",
      " selector for part 19: 230 vertices\n",
      " selector for part 20: 200 vertices\n",
      " selector for part 21: 198 vertices\n",
      " selector for part 22: 320 vertices\n",
      " selector for part 23: 546 vertices\n",
      " selector for part 24: 546 vertices\n",
      " selector for part 25: 33 vertices\n",
      " selector for part 26: 34 vertices\n",
      " selector for part 27: 54 vertices\n",
      " selector for part 28: 35 vertices\n",
      " selector for part 29: 30 vertices\n",
      " selector for part 30: 52 vertices\n",
      " selector for part 31: 23 vertices\n",
      " selector for part 32: 39 vertices\n",
      " selector for part 33: 52 vertices\n",
      " selector for part 34: 30 vertices\n",
      " selector for part 35: 38 vertices\n",
      " selector for part 36: 54 vertices\n",
      " selector for part 37: 21 vertices\n",
      " selector for part 38: 31 vertices\n",
      " selector for part 39: 54 vertices\n",
      " selector for part 40: 33 vertices\n",
      " selector for part 41: 34 vertices\n",
      " selector for part 42: 54 vertices\n",
      " selector for part 43: 35 vertices\n",
      " selector for part 44: 30 vertices\n",
      " selector for part 45: 52 vertices\n",
      " selector for part 46: 23 vertices\n",
      " selector for part 47: 39 vertices\n",
      " selector for part 48: 52 vertices\n",
      " selector for part 49: 30 vertices\n",
      " selector for part 50: 38 vertices\n",
      " selector for part 51: 54 vertices\n",
      " selector for part 52: 21 vertices\n",
      " selector for part 53: 31 vertices\n",
      " selector for part 54: 54 vertices\n",
      " selector for part 0: 434 vertices\n",
      " selector for part 1: 245 vertices\n",
      " selector for part 2: 226 vertices\n",
      " selector for part 3: 153 vertices\n",
      " selector for part 4: 195 vertices\n",
      " selector for part 5: 204 vertices\n",
      " selector for part 6: 186 vertices\n",
      " selector for part 7: 254 vertices\n",
      " selector for part 8: 254 vertices\n",
      " selector for part 9: 614 vertices\n",
      " selector for part 12: 149 vertices\n",
      " selector for part 13: 111 vertices\n",
      " selector for part 14: 123 vertices\n",
      " selector for part 15: 3345 vertices\n",
      " selector for part 16: 276 vertices\n",
      " selector for part 17: 278 vertices\n",
      " selector for part 18: 228 vertices\n",
      " selector for part 19: 230 vertices\n",
      " selector for part 20: 200 vertices\n",
      " selector for part 21: 198 vertices\n",
      " selector for part 22: 320 vertices\n",
      " selector for part 23: 546 vertices\n",
      " selector for part 24: 546 vertices\n",
      " selector for part 25: 33 vertices\n",
      " selector for part 26: 34 vertices\n",
      " selector for part 27: 54 vertices\n",
      " selector for part 28: 35 vertices\n",
      " selector for part 29: 30 vertices\n",
      " selector for part 30: 52 vertices\n",
      " selector for part 31: 23 vertices\n",
      " selector for part 32: 39 vertices\n",
      " selector for part 33: 52 vertices\n",
      " selector for part 34: 30 vertices\n",
      " selector for part 35: 38 vertices\n",
      " selector for part 36: 54 vertices\n",
      " selector for part 37: 21 vertices\n",
      " selector for part 38: 31 vertices\n",
      " selector for part 39: 54 vertices\n",
      " selector for part 40: 33 vertices\n",
      " selector for part 41: 34 vertices\n",
      " selector for part 42: 54 vertices\n",
      " selector for part 43: 35 vertices\n",
      " selector for part 44: 30 vertices\n",
      " selector for part 45: 52 vertices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " selector for part 46: 23 vertices\n",
      " selector for part 47: 39 vertices\n",
      " selector for part 48: 52 vertices\n",
      " selector for part 49: 30 vertices\n",
      " selector for part 50: 38 vertices\n",
      " selector for part 51: 54 vertices\n",
      " selector for part 52: 21 vertices\n",
      " selector for part 53: 31 vertices\n",
      " selector for part 54: 54 vertices\n",
      "NLF BodyFitter refinement succeeded.\n",
      " dimensions of cam_intrinsics: torch.Size([3, 3]), vertices: torch.Size([1, 10475, 3]) \n",
      " devices of cam_intrinsics: cuda:0, vertices: cuda:0 \n",
      " dimensions of projected points_2d: torch.Size([10475, 2]) \n",
      " dimensions of u: torch.Size([10475]), v: torch.Size([10475]), z: torch.Size([10475]) \n",
      " shapes of zbuffer points: torch.Size([4295, 3]), point cloud: (65916, 3) \n"
     ]
    }
   ],
   "source": [
    "final_params = fitter.fit(\n",
    "    keypoints_2d=None,\n",
    "    cam_intrinsics=cam_intrinsics.cpu().numpy(),\n",
    "    depth_map=depth_map,\n",
    "    point_cloud=point_cloud_array,\n",
    "    mask=masks[0].cpu().numpy(),\n",
    "    feet_mask=pipeline.masks_feet.cpu().numpy(),\n",
    "    conf_threshold=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2da1bc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " using vertices from params with shape: (10475, 3) \n",
      " length of vertices: 10475 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10475, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projected_image = fitter.project_mesh_on_image(final_params,processed_image, cam_intrinsics)\n",
    "cv2.imwrite(\"projected_mohamed_mesh_on_image.png\", projected_image)\n",
    "final_params['vertices'].shape\n",
    "\n",
    "\n",
    "# optimize_metric = fitter.run_metric_optimization(cam_intrinsics, point_cloud_array)\n",
    "# print(\"✓ NLF SMPL fitting complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c405b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12098, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcd = pcd.voxel_down_sample(0.01)\n",
    "point_cloud_array = np.asarray(pcd.points)\n",
    "point_cloud_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2877e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dimensions of cam_intrinsics: torch.Size([3, 3]), vertices: torch.Size([1, 10475, 3]) \n",
      " devices of cam_intrinsics: cpu, vertices: cuda:0 \n",
      " dimensions of projected points_2d: torch.Size([10475, 2]) \n",
      " dimensions of u: torch.Size([10475]), v: torch.Size([10475]), z: torch.Size([10475]) \n",
      " RANSAC scale: -0.0005 with 60 inliers \n"
     ]
    }
   ],
   "source": [
    "# cam_intrinsics_torch = torch.from_numpy(cam_intrinsics[0] if cam_intrinsics.ndim == 3 else cam_intrinsics).float().to(self.device)\n",
    "\n",
    "\n",
    "optimize_metric = fitter.run_metric_optimization(cam_intrinsics[0], point_cloud_array)\n",
    "# print(\"✓ NLF SMPL fitting complete\")\n",
    "# cam_intrinsics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5e4af943",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = fitter._infer_nlf(fitter.image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a58483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " shapes of vertices: torch.Size([1, 10475, 3]) , cam_intrinsics: torch.Size([1, 3, 3]) \n",
      " dimensions of cam_intrinsics: torch.Size([3, 3]), vertices: torch.Size([1, 10475, 3]) \n",
      " devices of cam_intrinsics: cpu, vertices: cuda:0 \n",
      " dimensions of projected points_2d: torch.Size([10475, 2]) \n",
      " dimensions of u: torch.Size([10475]), v: torch.Size([10475]), z: torch.Size([10475]) \n"
     ]
    }
   ],
   "source": [
    "vertices = pred[\"vertices3d\"][0].to(fitter.device)\n",
    "print(f\" shapes of vertices: {vertices.shape} , cam_intrinsics: {cam_intrinsics.shape} \")\n",
    "\n",
    "\n",
    "zbuffer_points = fitter.get_zbuffer(vertices, cam_intrinsics.squeeze(0))\n",
    "\n",
    "depth\n",
    "    ## ensure zbuffer are numpy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c7c27307",
   "metadata": {},
   "outputs": [],
   "source": [
    "zbuffer_points_np = zbuffer_points.detach().cpu().numpy()\n",
    "point_cloud = point_cloud_array\n",
    "zbuffer_points_np = fitter.center_pcd(zbuffer_points_np)\n",
    "point_cloud = fitter.center_pcd(point_cloud)\n",
    "# pass only the z values of both point clouds\n",
    "# z_depth = zbuffer_points_np[:, 2]\n",
    "# pc_depth = point_cloud[:, 2]\n",
    "\n",
    "# ## pass both point clouds to ransac scale\n",
    "# scale, inliers = fitter.ransac_scale(\n",
    "#     list(zip(z_depth, pc_depth)), iters=1000, tol=0.1\n",
    "# )\n",
    "# print(f\" RANSAC scale: {scale:.4f} with {inliers} inliers \")\n",
    "# # scale the vertices and joints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d923ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## conclusion what needs to be done next:\n",
    "\n",
    "## render the mesh before caluclating the zbuffer points\n",
    "## then calculate the zbuffer points from the rendered depth map\n",
    "## then do the ransac scaling between the zbuffer points and the point cloud\n",
    "\n",
    "\n",
    "### make sure that the mesh and the mask are aligned properly in the image\n",
    "### afterwards the scale would be more accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1c6c92f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.renderer import (\n",
    "    RasterizationSettings,\n",
    "    MeshRasterizer,\n",
    "    PerspectiveCameras,\n",
    ")\n",
    "from pytorch3d.structures import Meshes, Pointclouds\n",
    "from pytorch3d.loss import chamfer_distance\n",
    "\n",
    "device = vertices.device\n",
    "\n",
    "# Ensure point_cloud is on correct device\n",
    "if not isinstance(point_cloud, torch.Tensor):\n",
    "    point_cloud = torch.from_numpy(point_cloud).float().to(device)\n",
    "else:\n",
    "    point_cloud = point_cloud.to(device)\n",
    "\n",
    "# Create SMPLX mesh\n",
    "faces = torch.from_numpy(fitter.smplx_model.faces).long().to(device)\n",
    "mesh = Meshes(verts=[vertices.squeeze(0)], faces=[faces])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4913638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract H, W from intrinsics (assuming standard camera setup)\n",
    "cam_intrinsics = cam_intrinsics.squeeze(0)\n",
    "fx = -cam_intrinsics[0, 0] # flipped sign for PyTorch3D\n",
    "fy = -cam_intrinsics[1, 1]\n",
    "cx = cam_intrinsics[0, 2]\n",
    "cy = cam_intrinsics[1, 2]\n",
    "\n",
    "H = int(cy.item() * 2)\n",
    "W = int(cx.item() * 2)\n",
    "\n",
    "# Set up PyTorch3D camera\n",
    "cameras = PerspectiveCameras(\n",
    "    focal_length=((fx, fy),),\n",
    "    principal_point=((cx, cy),),\n",
    "    image_size=((H, W),),\n",
    "    device=device,\n",
    "    in_ndc=False\n",
    ")\n",
    "\n",
    "# Rasterization settings\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=(H, W),\n",
    "    blur_radius=0.0,\n",
    "    faces_per_pixel=1,\n",
    "    bin_size=0\n",
    ")\n",
    "\n",
    "# Rasterize mesh\n",
    "rasterizer = MeshRasterizer(\n",
    "    cameras=cameras,\n",
    "    raster_settings=raster_settings\n",
    ")\n",
    "\n",
    "fragments = rasterizer(mesh)\n",
    "rendered_depth = fragments.zbuf[0, ..., 0]  # [H, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "54ffbf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the rendered depth map as an image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imsave(\"rendered_depth_map.png\", rendered_depth.cpu().numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6bc53d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks[0]\n",
    "# depth_map = torch.from_numpy(depth_map).float().to(device)\n",
    "filtered_depth_map = depth_map * masks[0].float().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9b06d69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Rendered depth map stats: min=2369.3381, max=3158.9885, mean=2612.9963 \n",
      " Filtered depth map stats: min=1.2842, max=3.5501, mean=1.5162 \n"
     ]
    }
   ],
   "source": [
    "#print stats of filtered and rendered depth maps\n",
    "## reject any non-positive values from both depth maps before calculating stats\n",
    "rendered_depth = rendered_depth[rendered_depth > 0]\n",
    "filtered_depth_map = filtered_depth_map[filtered_depth_map > 0]\n",
    "print(f\" Rendered depth map stats: min={rendered_depth.min().item():.4f}, max={rendered_depth.max().item():.4f}, mean={rendered_depth.mean().item():.4f} \")\n",
    "print(f\" Filtered depth map stats: min={filtered_depth_map.min().item():.4f}, max={filtered_depth_map.max().item():.4f}, mean={filtered_depth_map.mean().item():.4f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a6731680",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imsave(\"filtered_depth.png\", filtered_depth_map.squeeze(0).cpu().numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "01ad04dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ransac_scale(\n",
    "        pairs: List[Tuple[float, float]], iters: int = 10000, tol: float = 0.05\n",
    "        ) -> Tuple[float, int]:\n",
    "        \"\"\"RANSAC for scale s in s * d_sfm = d_metric.\n",
    "\n",
    "        tol: relative tolerance |s*d_sfm - d_metric| <= tol * d_metric\n",
    "        Returns best_scale, inlier_count.\n",
    "        \"\"\"\n",
    "        if not pairs:\n",
    "            raise ValueError(\"No depth pairs provided.\")\n",
    "        pairs_arr = np.array(pairs, dtype=float)  # (N,2)\n",
    "        d_sfm = pairs_arr[:, 0]\n",
    "        d_met = pairs_arr[:, 1]\n",
    "        ratios = d_met / np.clip(d_sfm, 1e-9, None)\n",
    "        best_s = np.median(ratios)\n",
    "        best_inliers = 0\n",
    "        n = len(pairs)\n",
    "        for _ in range(iters):\n",
    "            i = random.randint(0, n - 1)\n",
    "            s_candidate = d_met[i] / max(d_sfm[i], 1e-9)\n",
    "            pred = s_candidate * d_sfm\n",
    "            err = np.abs(pred - d_met)\n",
    "            inliers = np.sum(err <= tol * np.maximum(d_met, 1e-6))\n",
    "            if inliers > best_inliers:\n",
    "                best_inliers = inliers\n",
    "                best_s = s_candidate\n",
    "\n",
    "        return best_s, best_inliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "06fa8a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RANSAC scale: 0.0006 with 1319 inliers \n"
     ]
    }
   ],
   "source": [
    "scale, inliers = ransac_scale(\n",
    "    list(zip(rendered_depth.cpu().numpy().flatten(), filtered_depth_map.cpu().numpy().flatten())), iters=1000, tol=0.01\n",
    ")\n",
    "print(f\" RANSAC scale: {scale:.4f} with {inliers} inliers \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b159c5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " shapes of vertices: torch.Size([3, 10475]) \n",
      " shapes of vertices_np: (10475, 3) \n",
      "\u001b[1;33m[Open3D WARNING] Write PLY failed: point cloud has 0 points.\u001b[0;m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save it using open3d 1723.387415908\n",
    "s = 0.0006\n",
    "import open3d as o3d\n",
    "zbuf_pcd_o3d = o3d.geometry.PointCloud()\n",
    "print(f\" shapes of vertices: {vertices.squeeze(0).transpose(0,1).shape} \")\n",
    "vertices_np = vertices.squeeze(0).detach().cpu().numpy() \n",
    "print(f\" shapes of vertices_np: {vertices_np.shape} \")\n",
    "vertices = vertices\n",
    "# zbuf_pcd_o3d.points = o3d.utility.Vector3dVector(vertices.squeeze(0).transpose(0,1))\n",
    "o3d.io.write_point_cloud(\"zbuffer_points.ply\", zbuf_pcd_o3d)\n",
    "pc_pcd_o3d = o3d.geometry.PointCloud()\n",
    "point_cloud_array\n",
    "pc_pcd_o3d.points = o3d.utility.Vector3dVector(point_cloud_array)\n",
    "o3d.io.write_point_cloud(\"point_cloud.ply\", pc_pcd_o3d)\n",
    "\n",
    "combined_pcd_o3d = o3d.geometry.PointCloud()\n",
    "combined_pcd_o3d.points = o3d.utility.Vector3dVector(\n",
    "    np.concatenate([vertices_np * s, point_cloud_array], axis=0)\n",
    ")\n",
    "o3d.io.write_point_cloud(\"combined_pcd.ply\", combined_pcd_o3d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c821062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "depth_metric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
