{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3852f13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Starting 3D Body Reconstruction Pipeline\n",
      "============================================================\n",
      "\n",
      "[1/5] Estimating depth and camera FOV...\n",
      "\u001b[97m[INFO ] using MLP layer as FFN\u001b[0m\n",
      "\u001b[97m[INFO ] Processed Images Done taking 0.0817408561706543 seconds. Shape:  torch.Size([1, 3, 336, 504])\u001b[0m\n",
      "\u001b[97m[INFO ] Model Forward Pass Done. Time: 0.10876703262329102 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Conversion to Prediction Done. Time: 0.0037078857421875 seconds\u001b[0m\n",
      "\u001b[97m[INFO ] Export Results Done. Time: 0.0009083747863769531 seconds\u001b[0m\n",
      "########### Using fov estimator: MoGe2...\n",
      "✓ Depth map shape: (336, 504)\n",
      "✓ Camera intrinsics estimated\n",
      "\n",
      "[2/5] Detecting 2D pose keypoints...\n",
      "\n",
      "image 1/1 /home/khater/pose-check/tom.jpg: 448x640 1 person, 8.7ms\n",
      "Speed: 4.8ms preprocess, 8.7ms inference, 4.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "✓ Detected 1 person(s)\n",
      "\n",
      "[3/5] Generating segmentation masks...\n",
      "✓ Generated 1 mask(s)\n",
      "\n",
      "[4/5] Creating 3D point cloud...\n",
      "✓ Point cloud created: 169344 points\n",
      "\n",
      "Cleaning up GPU memory...\n",
      "✓ GPU memory freed\n",
      "\n",
      "[5/5] Fitting SMPL-X model...\n",
      "\n",
      "============================================================\n",
      "SMPL-X FITTING\n",
      "============================================================\n",
      "Valid keypoints: 17/17\n",
      "\n",
      "Optimizing...\n",
      "Iter 0000 | Loss: 47747.5781 | Reproj: 477.4758 | Pose: 0.000000 | Shape: 0.000000\n",
      "Iter 0050 | Loss: 12575.2568 | Reproj: 125.7524 | Pose: 0.021994 | Shape: 0.161596\n",
      "Iter 0100 | Loss: 3309.7756 | Reproj: 33.0973 | Pose: 0.040401 | Shape: 0.491663\n",
      "Iter 0150 | Loss: 1184.2401 | Reproj: 11.8416 | Pose: 0.055438 | Shape: 0.786352\n",
      "Iter 0200 | Loss: 766.5685 | Reproj: 7.6646 | Pose: 0.065107 | Shape: 1.036510\n",
      "Iter 0250 | Loss: 572.6169 | Reproj: 5.7249 | Pose: 0.077694 | Shape: 1.308578\n",
      "Iter 0300 | Loss: 472.4755 | Reproj: 4.7231 | Pose: 0.090366 | Shape: 1.614708\n",
      "Iter 0350 | Loss: 424.0320 | Reproj: 4.2384 | Pose: 0.096134 | Shape: 1.947897\n",
      "Iter 0400 | Loss: 390.8012 | Reproj: 3.9057 | Pose: 0.099627 | Shape: 2.309183\n",
      "Iter 0450 | Loss: 362.2567 | Reproj: 3.6198 | Pose: 0.102452 | Shape: 2.710648\n",
      "\n",
      "============================================================\n",
      "FITTING COMPLETE\n",
      "============================================================\n",
      "Mesh exported to output/smplx_fitted.obj\n",
      "Mesh exported to output/smplx_fitted.ply\n",
      "✓ SMPL-X fitting complete\n",
      "✓ Meshes exported to output/\n",
      "\n",
      "Generating visualizations...\n",
      "Visualization saved to output/fitting_visualization.png\n",
      "⚠ Visualization generation failed: 'MetricSMPLFitter' object has no attribute 'phase1_params'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from main_pipeline import BodyReconstructionPipeline\n",
    "import config\n",
    "\n",
    "\n",
    "IMAGE_PATH = \"/home/khater/pose-check/tom.jpg\"\n",
    "GENDER = \"male\"\n",
    "OUTPUT_DIR = \"output\"\n",
    "\n",
    "# Initialize and run pipeline\n",
    "pipeline = BodyReconstructionPipeline(output_dir=OUTPUT_DIR)\n",
    "\n",
    "results = pipeline.run(\n",
    "    image_path=IMAGE_PATH,\n",
    "    gender=GENDER,\n",
    "    enable_visualization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f7bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = pipeline.fitter\n",
    "\n",
    "depth_map = results[\"depth_map\"]\n",
    "cam_intrinsics = results[\"cam_intrinsics\"]\n",
    "keypoints_2d = results[\"keypoints_2d\"]\n",
    "processed_image = results[\"processed_image\"]\n",
    "masks = results[\"masks\"]\n",
    "point_cloud_array = results[\"point_cloud\"]\n",
    "\n",
    "\n",
    "transl = fitter.fitted_params['transl'].clone().requires_grad_(True)\n",
    "global_orient = fitter.fitted_params['global_orient'].clone().requires_grad_(True)\n",
    "body_pose = fitter.fitted_params['body_pose'].clone().requires_grad_(True)\n",
    "betas = fitter.fitted_params['betas'].clone().requires_grad_(True)\n",
    "\n",
    "left_hand_pose = fitter.fitted_params['left_hand_pose']\n",
    "right_hand_pose = fitter.fitted_params['right_hand_pose']\n",
    "expression = fitter.fitted_params['expression']\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    [transl, global_orient, body_pose, betas],\n",
    "    lr=0.005\n",
    ")\n",
    "\n",
    "\n",
    "output = fitter.smplx_model(\n",
    "    betas=betas,\n",
    "    global_orient=global_orient,\n",
    "    body_pose=body_pose,\n",
    "    transl=transl,\n",
    "    left_hand_pose=left_hand_pose,\n",
    "    right_hand_pose=right_hand_pose,\n",
    "    expression=expression,\n",
    "    return_verts=True,\n",
    "    return_faces=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32326af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "verts = output.vertices\n",
    "faces = fitter.smplx_model.faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d84cd848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DepthNormalLoss:\n",
    "    def _compute_vertex_normals(self, vertices, faces):\n",
    "        \"\"\"\n",
    "        Compute vertex normals using weighted face normals.\n",
    "        vertices: [V, 3]\n",
    "        faces: [F, 3]\n",
    "        \"\"\"\n",
    "        # 1. Compute Face Normals\n",
    "        v0 = vertices[faces[:, 0]]\n",
    "        v1 = vertices[faces[:, 1]]\n",
    "        v2 = vertices[faces[:, 2]]\n",
    "        \n",
    "        edge1 = v1 - v0\n",
    "        edge2 = v2 - v0\n",
    "        # Cross product for face normal\n",
    "        face_normals = torch.cross(edge1, edge2, dim=1) # [F, 3]\n",
    "        \n",
    "        # 2. Scatter to Vertices (Simple uniform weighting)\n",
    "        # Note: Ideally area-weighted, but uniform is faster/differentiable\n",
    "        vertex_normals = torch.zeros_like(vertices)\n",
    "        \n",
    "        # Add face normal to each vertex in the face\n",
    "        for i in range(3):\n",
    "            vertex_normals.index_add_(0, faces[:, i], face_normals)\n",
    "            \n",
    "        # 3. Normalize\n",
    "        vertex_normals = F.normalize(vertex_normals, dim=1, eps=1e-6)\n",
    "        return vertex_normals\n",
    "\n",
    "    def _depth_to_normals(self, depth, intrinsics):\n",
    "        \"\"\"\n",
    "        Compute normal map from depth map using back-projection and gradients.\n",
    "        depth: [H, W]\n",
    "        intrinsics: [3, 3]\n",
    "        \"\"\"\n",
    "        H, W = depth.shape\n",
    "        fx, fy = intrinsics[0, 0], intrinsics[1, 1]\n",
    "        cx, cy = intrinsics[0, 2], intrinsics[1, 2]\n",
    "\n",
    "        # Create grid\n",
    "        y, x = torch.meshgrid(torch.arange(H, device=depth.device), \n",
    "                              torch.arange(W, device=depth.device), indexing='ij')\n",
    "        \n",
    "        # Back-project to 3D (approximate for gradients)\n",
    "        # X = (x - cx) * Z / fx\n",
    "        # Y = (y - cy) * Z / fy\n",
    "        X = (x - cx) * depth / fx\n",
    "        Y = (y - cy) * depth / fy\n",
    "        XYZ = torch.stack([X, Y, depth], dim=-1) # [H, W, 3]\n",
    "\n",
    "        # Compute gradients (central difference)\n",
    "        # Pad to handle borders\n",
    "        padded = F.pad(XYZ, (0, 0, 1, 1, 1, 1), mode='replicate')\n",
    "        \n",
    "        # d/dy\n",
    "        v_up   = padded[:-2, 1:-1, :]\n",
    "        v_down = padded[2:, 1:-1, :]\n",
    "        dy = v_down - v_up\n",
    "\n",
    "        # d/dx\n",
    "        v_left  = padded[1:-1, :-2, :]\n",
    "        v_right = padded[1:-1, 2:, :]\n",
    "        dx = v_right - v_left\n",
    "\n",
    "        # Cross product: dx x dy gives normal\n",
    "        cross = torch.cross(dx, dy, dim=-1)\n",
    "        \n",
    "        # Normalize (and flip sign if pointing away, usually Z should be positive)\n",
    "        # In standard camera, normals point towards camera (-Z) or away (+Z)? \n",
    "        # Standard: Surface normal usually points opposite to viewing direction.\n",
    "        normals = F.normalize(cross, dim=-1)\n",
    "        return normals\n",
    "\n",
    "    def compute_depth_loss(\n",
    "            self,\n",
    "            vertices: torch.Tensor,        # [V, 3]\n",
    "            faces: torch.Tensor,           # [F, 3] (ADDED)\n",
    "            cam_intrinsics: torch.Tensor,  # [3, 3]\n",
    "            depth_map: torch.Tensor,       # [H, W]\n",
    "            mask: torch.Tensor,            # [H, W]\n",
    "            max_verts: int = 3000,\n",
    "            it: int = 0,\n",
    "            normal_weight: float = 0.1     # (ADDED)\n",
    "        ) -> torch.Tensor:\n",
    "            \n",
    "            device = vertices.device\n",
    "            H, W = depth_map.shape\n",
    "\n",
    "            # ----------------------------------\n",
    "            # 0. Pre-compute Normals\n",
    "            # ----------------------------------\n",
    "            # Source Normals (Vertex)\n",
    "            v_normals = self._compute_vertex_normals(vertices, faces)\n",
    "            \n",
    "            # Target Normals (from Ground Truth Depth)\n",
    "            # You might want to cache this if depth_map is static\n",
    "            gt_normals_map = self._depth_to_normals(depth_map, cam_intrinsics) # [H, W, 3]\n",
    "\n",
    "            # ----------------------------------\n",
    "            # 1. Project vertices\n",
    "            # ----------------------------------\n",
    "            points = self._project_points(vertices, cam_intrinsics)\n",
    "            u = points[:, 0].long()\n",
    "            v = points[:, 1].long()\n",
    "            z = vertices[:, 2]\n",
    "\n",
    "            # ----------------------------------\n",
    "            # 2. Filter: Image bounds\n",
    "            # ----------------------------------\n",
    "            inside = (u >= 0) & (u < W) & (v >= 0) & (v < H)\n",
    "            u, v, z = u[inside], v[inside], z[inside]\n",
    "            # Also filter normals\n",
    "            v_normals_subset = v_normals[inside]\n",
    "\n",
    "            if z.numel() == 0:\n",
    "                return torch.zeros((), device=device)\n",
    "\n",
    "            # ----------------------------------\n",
    "            # 3. Filter: Mask\n",
    "            # ----------------------------------\n",
    "            linear_idx = v * W + u\n",
    "            mask_flat = mask.reshape(-1)\n",
    "            mask_values = mask_flat[linear_idx]\n",
    "            mask_valid = mask_values > 0.5\n",
    "\n",
    "            u, v, z = u[mask_valid], v[mask_valid], z[mask_valid]\n",
    "            v_normals_subset = v_normals_subset[mask_valid] # Keep normals in sync\n",
    "\n",
    "            if z.numel() == 0:\n",
    "                return torch.zeros((), device=device)\n",
    "\n",
    "            # ----------------------------------\n",
    "            # 4. Z-buffer (Min Z per pixel)\n",
    "            # ----------------------------------\n",
    "            pixel_ids = v * W + u\n",
    "            unique_ids, inverse = torch.unique(pixel_ids, return_inverse=True)\n",
    "\n",
    "            # Compute min Z\n",
    "            z_min = torch.full((unique_ids.shape[0],), float('inf'), device=device)\n",
    "            z_min.scatter_reduce_(0, inverse, z, reduce='amin')\n",
    "            \n",
    "            # ----------------------------------\n",
    "            # 5. Identify \"Winning\" Fragments\n",
    "            # ----------------------------------\n",
    "            # We need to find which vertices actually contributed to z_min \n",
    "            # to pick the correct source normals.\n",
    "            \n",
    "            # Broadcast min values back to original list size\n",
    "            z_min_expanded = z_min[inverse]\n",
    "            \n",
    "            # Create a mask for vertices that are visible (closest)\n",
    "            # Use a small epsilon for float comparison\n",
    "            is_closest = torch.abs(z - z_min_expanded) < 1e-5\n",
    "            \n",
    "            # Filter our attributes to just the visible fragments\n",
    "            # Note: This might still contain duplicates if two vertices have \n",
    "            # exact same depth at same pixel, but typically rare.\n",
    "            visible_z = z[is_closest]\n",
    "            visible_u = u[is_closest]\n",
    "            visible_v = v[is_closest]\n",
    "            visible_src_normals = v_normals_subset[is_closest]\n",
    "\n",
    "            # ----------------------------------\n",
    "            # 6. Compute Losses\n",
    "            # ----------------------------------\n",
    "            \n",
    "            # A. Depth Loss\n",
    "            # Lookup GT depth at the visible pixels\n",
    "            depth_gt = depth_map[visible_v, visible_u]\n",
    "            loss_depth = torch.mean((visible_z - depth_gt) ** 2)\n",
    "\n",
    "            # B. Normal Loss\n",
    "            # Lookup GT normals at the visible pixels\n",
    "            target_normals = gt_normals_map[visible_v, visible_u] # [N, 3]\n",
    "            \n",
    "            # Cosine Similarity Loss: 1 - cos(theta)\n",
    "            # Dot product of normalized vectors\n",
    "            dot_prod = torch.sum(visible_src_normals * target_normals, dim=1)\n",
    "            \n",
    "            # Clamp for stability in case of numerical noise > 1.0\n",
    "            dot_prod = torch.clamp(dot_prod, -1.0, 1.0)\n",
    "            \n",
    "            # We want vectors to align, so maximize dot product -> minimize (1 - dot)\n",
    "            loss_normal = torch.mean(1.0 - dot_prod)\n",
    "\n",
    "            # ----------------------------------\n",
    "            # Debug Visualization\n",
    "            # ----------------------------------\n",
    "            if it % 50 == 0:\n",
    "                print(f\"Depth Loss: {loss_depth.item():.4f}, Normal Loss: {loss_normal.item():.4f}\")\n",
    "                # ... (existing visualization code) ...\n",
    "\n",
    "            return loss_depth + (normal_weight * loss_normal)\n",
    "    \n",
    "\n",
    "    def _project_points(\n",
    "        self,\n",
    "        points_3d: torch.Tensor,\n",
    "        cam_intrinsics: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Project 3D points to 2D using camera intrinsics.\n",
    "        \n",
    "        Args:\n",
    "            points_3d: [N, 3] 3D points\n",
    "            cam_intrinsics: [3, 3] camera intrinsics\n",
    "            \n",
    "        Returns:\n",
    "            points_2d: [N, 2] projected 2D points\n",
    "        \"\"\"\n",
    "        fx = cam_intrinsics[0, 0]\n",
    "        fy = cam_intrinsics[1, 1]\n",
    "        cx = cam_intrinsics[0, 2]\n",
    "        cy = cam_intrinsics[1, 2]\n",
    "        \n",
    "        # Perspective projection\n",
    "        x_2d = fx * points_3d[:, 0] / (points_3d[:, 2] + 1e-6) + cx\n",
    "        y_2d = -fy * points_3d[:, 1] / (points_3d[:, 2] + 1e-6) + cy\n",
    "        \n",
    "        return torch.stack([x_2d, y_2d], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94e3fff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing depth loss...\n",
      " verts shape: torch.Size([1, 10475, 3]), cam_intrinsics shape: torch.Size([3, 3]), depth_map shape: (336, 504), mask shape: torch.Size([1, 336, 504])\n"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_depth_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvertices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfaces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m# <--- NEW: Must pass faces\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcam_intrinsics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcam_intrinsics\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdepth_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdepth_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmasks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormal_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# <--- NEW: Control normal influence\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 96\u001b[0m, in \u001b[0;36mDepthNormalLoss.compute_depth_loss\u001b[0;34m(self, vertices, faces, cam_intrinsics, depth_map, mask, max_verts, it, normal_weight)\u001b[0m\n\u001b[1;32m     90\u001b[0m H, W \u001b[38;5;241m=\u001b[39m depth_map\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# ----------------------------------\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# 0. Pre-compute Normals\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# ----------------------------------\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Source Normals (Vertex)\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m v_normals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_vertex_normals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvertices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfaces\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Target Normals (from Ground Truth Depth)\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# You might want to cache this if depth_map is static\u001b[39;00m\n\u001b[1;32m    100\u001b[0m gt_normals_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_depth_to_normals(depth_map, cam_intrinsics) \u001b[38;5;66;03m# [H, W, 3]\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 12\u001b[0m, in \u001b[0;36mDepthNormalLoss._compute_vertex_normals\u001b[0;34m(self, vertices, faces)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03mCompute vertex normals using weighted face normals.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03mvertices: [V, 3]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mfaces: [F, 3]\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 1. Compute Face Normals\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m v0 \u001b[38;5;241m=\u001b[39m \u001b[43mvertices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfaces\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     13\u001b[0m v1 \u001b[38;5;241m=\u001b[39m vertices[faces[:, \u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m     14\u001b[0m v2 \u001b[38;5;241m=\u001b[39m vertices[faces[:, \u001b[38;5;241m2\u001b[39m]]\n",
      "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing depth loss...\")\n",
    "print(f\" verts shape: {verts.shape}, cam_intrinsics shape: {cam_intrinsics[0].shape}, depth_map shape: {depth_map.shape}, mask shape: {masks[0].shape}\")\n",
    "# turn depth map to torch tensor\n",
    "loss_fn = DepthNormalLoss()\n",
    "\n",
    "# optimizer = torch.optim.Adam([verts], lr=0.01)\n",
    "\n",
    "for it in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = loss_fn.compute_depth_loss(\n",
    "        vertices=verts,\n",
    "        faces=faces,                  # <--- NEW: Must pass faces\n",
    "        cam_intrinsics=cam_intrinsics[0],\n",
    "        depth_map=depth_map,\n",
    "        mask=masks[0],\n",
    "        it=it,\n",
    "        normal_weight=0.1             # <--- NEW: Control normal influence\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "107801c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5794, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a7a7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " shape of u: torch.Size([3000]), v: torch.Size([3000]), z: torch.Size([3000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vertices = verts\n",
    "mask = masks[0]\n",
    "max_verts = 3000\n",
    "eps = 1e-6\n",
    "device = vertices.device\n",
    "H, W = depth_map.shape\n",
    "\n",
    "# Subsample vertices\n",
    "V = vertices.shape[0]\n",
    "if V > max_verts:\n",
    "    idx = torch.randperm(V, device=device)[:max_verts]\n",
    "    verts = vertices[idx]\n",
    "else:\n",
    "    verts = vertices\n",
    "\n",
    "# Keep vertices in front of camera\n",
    "verts = verts[verts[:, 2] > eps]\n",
    "# if verts.numel() == 0:\n",
    "#     return torch.zeros((), device=device)\n",
    "if cam_intrinsics.ndim == 3:\n",
    "    cam_intrinsics = cam_intrinsics[0]\n",
    "\n",
    "\n",
    "fx, fy = cam_intrinsics[0, 0], cam_intrinsics[1, 1]\n",
    "cx, cy = cam_intrinsics[0, 2], cam_intrinsics[1, 2]\n",
    "\n",
    "u = fx * verts[:, 0] / (verts[:, 2] + eps) + cx\n",
    "v = -fy * verts[:, 1] / (verts[:, 2] + eps) + cy\n",
    "\n",
    "u = u.long()\n",
    "v = v.long()\n",
    "\n",
    "# Image bounds\n",
    "valid = (\n",
    "    (u >= 0) & (u < W) &\n",
    "    (v >= 0) & (v < H)\n",
    ")\n",
    "\n",
    "u = u[valid]\n",
    "v = v[valid]\n",
    "z = verts[valid, 2]\n",
    "\n",
    "if z.numel() == 0:\n",
    "    print(\"No valid vertices after image bounds check.\")\n",
    "u = u.long()\n",
    "v = v.long()\n",
    "# print(f\" shape of mask: {mask.shape}\")\n",
    "\n",
    "\n",
    "print(f\" shape of u: {u.shape}, v: {v.shape}, z: {z.shape}\")\n",
    "# Apply segmentation mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da139b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask shape: torch.Size([1, 336, 504]), expected: [336, 504]\n",
      "u range: [200, 298], v range: [13, 302]\n"
     ]
    }
   ],
   "source": [
    "## mask is a binary mask of shape [H,W]\n",
    "## for each (u,v) coordinate, check if mask[v,u] is True\n",
    "\n",
    "# First, verify mask shape and clamp indices to be safe\n",
    "print(f\"mask shape: {mask.shape}, expected: [{H}, {W}]\")\n",
    "print(f\"u range: [{u.min()}, {u.max()}], v range: [{v.min()}, {v.max()}]\")\n",
    "\n",
    "# Clamp indices to valid range (defensive programming)\n",
    "v_clamped = torch.clamp(v, 0, H - 1)\n",
    "u_clamped = torch.clamp(u, 0, W - 1)\n",
    "\n",
    "mask_valid = mask[v_clamped, u_clamped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4fb554",
   "metadata": {},
   "outputs": [
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m mask_valid shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmask_valid\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, sum: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmask_valid\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DepthNormalLoss:\n",
    "    def _compute_vertex_normals(self, vertices, faces):\n",
    "        \"\"\"\n",
    "        Compute vertex normals using weighted face normals.\n",
    "        vertices: [V, 3]\n",
    "        faces: [F, 3]\n",
    "        \"\"\"\n",
    "        # 1. Compute Face Normals\n",
    "        v0 = vertices[faces[:, 0]]\n",
    "        v1 = vertices[faces[:, 1]]\n",
    "        v2 = vertices[faces[:, 2]]\n",
    "        \n",
    "        edge1 = v1 - v0\n",
    "        edge2 = v2 - v0\n",
    "        # Cross product for face normal\n",
    "        face_normals = torch.cross(edge1, edge2, dim=1) # [F, 3]\n",
    "        \n",
    "        # 2. Scatter to Vertices (Simple uniform weighting)\n",
    "        # Note: Ideally area-weighted, but uniform is faster/differentiable\n",
    "        vertex_normals = torch.zeros_like(vertices)\n",
    "        \n",
    "        # Add face normal to each vertex in the face\n",
    "        for i in range(3):\n",
    "            vertex_normals.index_add_(0, faces[:, i], face_normals)\n",
    "            \n",
    "        # 3. Normalize\n",
    "        vertex_normals = F.normalize(vertex_normals, dim=1, eps=1e-6)\n",
    "        return vertex_normals\n",
    "\n",
    "    def _depth_to_normals(self, depth, intrinsics):\n",
    "        \"\"\"\n",
    "        Compute normal map from depth map using back-projection and gradients.\n",
    "        depth: [H, W]\n",
    "        intrinsics: [3, 3]\n",
    "        \"\"\"\n",
    "        H, W = depth.shape\n",
    "        fx, fy = intrinsics[0, 0], intrinsics[1, 1]\n",
    "        cx, cy = intrinsics[0, 2], intrinsics[1, 2]\n",
    "\n",
    "        # Create grid\n",
    "        y, x = torch.meshgrid(torch.arange(H, device=depth.device), \n",
    "                              torch.arange(W, device=depth.device), indexing='ij')\n",
    "        \n",
    "        # Back-project to 3D (approximate for gradients)\n",
    "        # X = (x - cx) * Z / fx\n",
    "        # Y = (y - cy) * Z / fy\n",
    "        X = (x - cx) * depth / fx\n",
    "        Y = (y - cy) * depth / fy\n",
    "        XYZ = torch.stack([X, Y, depth], dim=-1) # [H, W, 3]\n",
    "\n",
    "        # Compute gradients (central difference)\n",
    "        # Pad to handle borders\n",
    "        padded = F.pad(XYZ, (0, 0, 1, 1, 1, 1), mode='replicate')\n",
    "        \n",
    "        # d/dy\n",
    "        v_up   = padded[:-2, 1:-1, :]\n",
    "        v_down = padded[2:, 1:-1, :]\n",
    "        dy = v_down - v_up\n",
    "\n",
    "        # d/dx\n",
    "        v_left  = padded[1:-1, :-2, :]\n",
    "        v_right = padded[1:-1, 2:, :]\n",
    "        dx = v_right - v_left\n",
    "\n",
    "        # Cross product: dx x dy gives normal\n",
    "        cross = torch.cross(dx, dy, dim=-1)\n",
    "        \n",
    "        # Normalize (and flip sign if pointing away, usually Z should be positive)\n",
    "        # In standard camera, normals point towards camera (-Z) or away (+Z)? \n",
    "        # Standard: Surface normal usually points opposite to viewing direction.\n",
    "        normals = F.normalize(cross, dim=-1)\n",
    "        return normals\n",
    "\n",
    "    def compute_depth_loss(\n",
    "            self,\n",
    "            vertices: torch.Tensor,        # [V, 3]\n",
    "            faces: torch.Tensor,           # [F, 3] (ADDED)\n",
    "            cam_intrinsics: torch.Tensor,  # [3, 3]\n",
    "            depth_map: torch.Tensor,       # [H, W]\n",
    "            mask: torch.Tensor,            # [H, W]\n",
    "            max_verts: int = 3000,\n",
    "            it: int = 0,\n",
    "            normal_weight: float = 0.1     # (ADDED)\n",
    "        ) -> torch.Tensor:\n",
    "            \n",
    "            device = vertices.device\n",
    "            H, W = depth_map.shape\n",
    "\n",
    "            # ----------------------------------\n",
    "            # 0. Pre-compute Normals\n",
    "            # ----------------------------------\n",
    "            # Source Normals (Vertex)\n",
    "            v_normals = self._compute_vertex_normals(vertices, faces)\n",
    "            \n",
    "            # Target Normals (from Ground Truth Depth)\n",
    "            # You might want to cache this if depth_map is static\n",
    "            gt_normals_map = self._depth_to_normals(depth_map, cam_intrinsics) # [H, W, 3]\n",
    "\n",
    "            # ----------------------------------\n",
    "            # 1. Project vertices\n",
    "            # ----------------------------------\n",
    "            points = self._project_points(vertices, cam_intrinsics)\n",
    "            u = points[:, 0].long()\n",
    "            v = points[:, 1].long()\n",
    "            z = vertices[:, 2]\n",
    "\n",
    "            # ----------------------------------\n",
    "            # 2. Filter: Image bounds\n",
    "            # ----------------------------------\n",
    "            inside = (u >= 0) & (u < W) & (v >= 0) & (v < H)\n",
    "            u, v, z = u[inside], v[inside], z[inside]\n",
    "            # Also filter normals\n",
    "            v_normals_subset = v_normals[inside]\n",
    "\n",
    "            if z.numel() == 0:\n",
    "                return torch.zeros((), device=device)\n",
    "\n",
    "            # ----------------------------------\n",
    "            # 3. Filter: Mask\n",
    "            # ----------------------------------\n",
    "            linear_idx = v * W + u\n",
    "            mask_flat = mask.reshape(-1)\n",
    "            mask_values = mask_flat[linear_idx]\n",
    "            mask_valid = mask_values > 0.5\n",
    "\n",
    "            u, v, z = u[mask_valid], v[mask_valid], z[mask_valid]\n",
    "            v_normals_subset = v_normals_subset[mask_valid] # Keep normals in sync\n",
    "\n",
    "            if z.numel() == 0:\n",
    "                return torch.zeros((), device=device)\n",
    "\n",
    "            # ----------------------------------\n",
    "            # 4. Z-buffer (Min Z per pixel)\n",
    "            # ----------------------------------\n",
    "            pixel_ids = v * W + u\n",
    "            unique_ids, inverse = torch.unique(pixel_ids, return_inverse=True)\n",
    "\n",
    "            # Compute min Z\n",
    "            z_min = torch.full((unique_ids.shape[0],), float('inf'), device=device)\n",
    "            z_min.scatter_reduce_(0, inverse, z, reduce='amin')\n",
    "            \n",
    "            # ----------------------------------\n",
    "            # 5. Identify \"Winning\" Fragments\n",
    "            # ----------------------------------\n",
    "            # We need to find which vertices actually contributed to z_min \n",
    "            # to pick the correct source normals.\n",
    "            \n",
    "            # Broadcast min values back to original list size\n",
    "            z_min_expanded = z_min[inverse]\n",
    "            \n",
    "            # Create a mask for vertices that are visible (closest)\n",
    "            # Use a small epsilon for float comparison\n",
    "            is_closest = torch.abs(z - z_min_expanded) < 1e-5\n",
    "            \n",
    "            # Filter our attributes to just the visible fragments\n",
    "            # Note: This might still contain duplicates if two vertices have \n",
    "            # exact same depth at same pixel, but typically rare.\n",
    "            visible_z = z[is_closest]\n",
    "            visible_u = u[is_closest]\n",
    "            visible_v = v[is_closest]\n",
    "            visible_src_normals = v_normals_subset[is_closest]\n",
    "\n",
    "            # ----------------------------------\n",
    "            # 6. Compute Losses\n",
    "            # ----------------------------------\n",
    "            \n",
    "            # A. Depth Loss\n",
    "            # Lookup GT depth at the visible pixels\n",
    "            depth_gt = depth_map[visible_v, visible_u]\n",
    "            loss_depth = torch.mean((visible_z - depth_gt) ** 2)\n",
    "\n",
    "            # B. Normal Loss\n",
    "            # Lookup GT normals at the visible pixels\n",
    "            target_normals = gt_normals_map[visible_v, visible_u] # [N, 3]\n",
    "            \n",
    "            # Cosine Similarity Loss: 1 - cos(theta)\n",
    "            # Dot product of normalized vectors\n",
    "            dot_prod = torch.sum(visible_src_normals * target_normals, dim=1)\n",
    "            \n",
    "            # Clamp for stability in case of numerical noise > 1.0\n",
    "            dot_prod = torch.clamp(dot_prod, -1.0, 1.0)\n",
    "            \n",
    "            # We want vectors to align, so maximize dot product -> minimize (1 - dot)\n",
    "            loss_normal = torch.mean(1.0 - dot_prod)\n",
    "\n",
    "            # ----------------------------------\n",
    "            # Debug Visualization\n",
    "            # ----------------------------------\n",
    "            if it % 50 == 0:\n",
    "                print(f\"Depth Loss: {loss_depth.item():.4f}, Normal Loss: {loss_normal.item():.4f}\")\n",
    "                # ... (existing visualization code) ...\n",
    "\n",
    "            return loss_depth + (normal_weight * loss_normal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "depth_metric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
